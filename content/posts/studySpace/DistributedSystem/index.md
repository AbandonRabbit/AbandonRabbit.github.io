---
date: '2025-05-10T11:45:54+08:00'
draft: false
title: 'MIT6.824_分布式系统'
seriesOpened: false #s是否开启系列
# series: [""] #属于的系列 
# series_order: 0  #系列编号
showSummary: ["MIT6.824_分布式系统"] #摘要信息
tags: ["分布式系统"]
Categories: ["学习笔记"]
layoutBackgroundBlur: false #向下滚动主页时，是否模糊背景图。
layoutBackgroundHeaderSpace: true #在标题和正文之间添加空白区域间隔。

---

多台计算机进行协作，完成同一项任务，在设计一个系统时，先考虑是否可以在单机上面运行，如果可以，那么就应该在单机上面解决，在考虑一个问题时，需要知道他是否可以在不需要分布式系统的单机上解决，使用单机解决问题通常比分布式系统简单很多，分布式系统会让问题变的复杂。

在设计分布式系统时，可以分为三个方面，存储、通信、计算。

分布式系统更高的目标是具有可扩展的速度提升，2 × 计算资源数量，可以得到两倍的性能或吞吐量提升。

当有 1 亿个 用户同时访问一个 web 服务时，单机是肯定扛不住的，这时就需要增加 web 服务器，但是 web 服务器也不是可以无限增加的，如果增加了 100 台 web 服务器，访问的都是同一个 DB 服务器，这时再增加 web 服务器提升的性能就很小，如果还要提升性能，就需要增加 DB 服务器。

我们把一个很罕见的错误转变成一个在一千台的分布式系统中常见的错误，甚至在这个集群中一直在发生的错误。一台机器可以稳定运行很长时间，但是一千台机器同时运行，那么每天总会有某台机器出现故障，各个地方总会出现一些小问题，比如散热风扇不转，有人把网线踩掉。这意味着在分布式系统的扩容，需要把一个很小的问题变成一个持续不断的问题。意味着对于错误而言，正确回复或者掩盖错误，以及继续处理的能力必须要在架构设计时就建立，因为错误总会发生。

**容错** 

1. 可用性：经过精心设计，在某个特定的错误场景下还能提供完整的服务，某些系统建立了冗余的服务，比如做了两个备份，即便一个备份发生问题，可能另一个还能提供服务，因此可用性是建立在特定的错误类型上的。
2. 自身的可恢复性：如果出现问题，会停止工作，等被人修复之后，如果没有发生更糟糕的问题，系统可以继续正常运行，比可用性弱一些，在故障被修复之前，他是不会做任何事情，这一切是建立在没有损失正确性的前提下。

**容错解决工具**中一个是**非易失性存储**，但是代价是很大的，第一是需要摆动磁盘臂，等待磁盘盘面旋转，即便现在闪存的性能很高，但是还会遇到很多性能不够的情况。另一个是**复制**，两台系统本应该有同样的副本，但是这两个副本总是会有意外的，偏离同步状态，不再正确。

**一致性：**

1. 强一致性：保证所有的 Get 操作读到最新的数据。*需要更昂贵的通信*
2. 弱一致性：不保证所有的 Get 操作读到最新的数据。

## MapReduce模型

大数据三驾马车的其中之一。

主要思想是将大的任务拆分成小的任务，分发到不同的机器上进行并行计算。

大致可以分为 **Map**、**Shuffle**、**Reduce** 三个过程。

**Map**：根据任务量大小分为多个 Map ，每个 Map 任务会读取源数据，会最终生成一个 `< K , v >` 键值对，然后根据 K 计算所属分区，并生成一个逻辑标识 P ，表示应该应该去到的 Reduce。

**Shuffle**：过程包含在 Map 端和 Reduce 端， Map 端的 Shuffle 会对数据进行一个排序，得到一个有序的文件，该文件按照分区排序，并且每个分区的键值对都按照 K 的值进行升序排序， Reduce 端的 Shuffle 拉取属于自己分区的数据，并进行一个合并排序。

**Reduce**：根据业务需求对数据处理，并输出结果。

## Big Storage
分布式存储主要包括**并行性能**、**容错**、**复制**、**一致性**。
### 难点
将数据分片到各个服务器上，可以并行的从多台服务器读取，提升性能。这样会导致错误的增加，每天每小时都会有服务器宕机，所以需要一个自动化的**容错系统**。


### 容错系统
**复制**：有两个或者三个副本，需要考虑数据一致性，会损失性能。**数据一致性也要保证操作一致性**
   > 两台数据服务器，如果同时有多个请求，要保证处理这些请求的顺序也是相同的。  
   > 强一致性的性能代价是很大的，基本上都考虑弱一致性，把一致性控制在合理的范围之内

## GFS_分布式文件系统设计

为大型顺序文件读写以多种方式定制的，只处理大型文件的顺序访问，而不是随机访问，用性能换一致性，用复制换稳定。

**Master节点**：
存储从文件名到数据存储位置的映射，并管理命名和追踪 chunk 的位置

**ChunkServer节点**：
块服务器，存储实际的数据，每个 Chunk 是固定大小（64MB）

每个 Primary 节点有一个租约时间，这个租约时间是由 Master 发放的。 

Master 存储了两张表
1. FileName 到 Chunk 的映射：  
   记录每个文件路径（如 /data/log.txt）对应的一组 Chunk ID，体现整个文件系统的命名空间和逻辑结构。
2. Chunk ID 到元数据的映射：  
   其中有一项是 Chunk Server 的列表，这些 Chunk Server 保存数据的副本。另一项为 Chunk 的版本号，对于 Chunk 的读写操作必须在 Chunk Primary —— 主节点上顺序化，

在 Master 节点中，读取数据从内存中读，写操作都需要保证写入磁盘，任何数据变更都会在磁盘的日志追加一条，并定期创建 CheckPoint
> Chunk Server 列表不会保存在磁盘上，Master 重启后会询问 Chunk Server都存储了哪些 Chunk

当 Master 创建了一个新的 Chunk 或者新的 Primary 被指派导致版本号发生改变时 Master 都必须追加一点记录到日志中，内容为 添加了一个新的 Chunk 或修改了版本号

某些时候会额外创建一份状态快照到磁盘。当重启的时候会回到最近的 CheckPoint 的位置，重演 CheckPoint 之后的日志就可以

### Read操作
1. 客户机请求 Master 节点
2. Master 查表，获取多个 ChunkServer的位置
3. 客户机选一个 ChunkServer 根据延迟或就近挑选一个副本脸上
4. 发送请求给 ChunkServer ，包含 chunk ID + 读取偏移 + 长度。

> **Q:**  如果读取的文件刚好是两个 chunkserver的头部和尾部怎么办，这一段刚好跨了 chunk A 的尾巴 + chunk B 的开头  
> **A:**  分两段读取，Master 会返回 Chunk ID 列表和每个 Chunk 副本的位置，然后客户端识别出跨 Chunk ，最后分别发起两个读取请求，一个读 Chunk A 的尾部，另一个请求读 Chunk B 的头部，然后由客户端拼接返回。

### Write操作
1. 客户机请求 Master 节点，获取到 Chunk 位置和当前 Primary ，如果没有 Primary ，会选一个并签发租约
2. 客户机按照流水线的方式 Push 到所有的副本，这时还没有写入磁盘，这是缓存在内存中
3. 客户机发起写请求给 Primary 
4. Primary 发出写命令给其他副本，带上操作序号
5. 所有副本收到后，按顺序写入磁盘
6. 副本写入之后，按顺序写入磁盘
7. 所有副本写完之后，Primary 收集确认，并回复客户机成功
